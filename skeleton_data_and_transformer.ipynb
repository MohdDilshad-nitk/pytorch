{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBH/AxeqQGHP7CPJ9G2WXf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohdDilshad-nitk/pytorch/blob/main/skeleton_data_and_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ine2ucYCjXJ7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4wT2sJVnKMR",
        "outputId": "bd2db69d-8946-4149-fe14-cfe98aed3c26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Data.zip\n",
            "replace Data/Person001/1.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: Data/Person001/1.txt    \n",
            "  inflating: Data/Person001/2.txt    \n",
            "  inflating: Data/Person001/3.txt    \n",
            "  inflating: Data/Person001/4.txt    \n",
            "  inflating: Data/Person001/5.txt    \n",
            "  inflating: Data/Person002/1.txt    \n",
            "  inflating: Data/Person002/2.txt    \n",
            "  inflating: Data/Person002/3.txt    \n",
            "  inflating: Data/Person002/4.txt    \n",
            "  inflating: Data/Person003/1.txt    \n",
            "  inflating: Data/Person003/2.txt    \n",
            "  inflating: Data/Person003/3.txt    \n",
            "  inflating: Data/Person003/4.txt    \n",
            "  inflating: Data/Person003/5.txt    \n",
            "  inflating: Data/Person003/6.txt    \n",
            "  inflating: Data/Person004/1.txt    \n",
            "  inflating: Data/Person004/2.txt    \n",
            "  inflating: Data/Person004/3.txt    \n",
            "  inflating: Data/Person004/4.txt    \n",
            "  inflating: Data/Person004/5.txt    \n",
            "  inflating: Data/Person005/1.txt    \n",
            "  inflating: Data/Person005/2.txt    \n",
            "  inflating: Data/Person005/3.txt    \n",
            "  inflating: Data/Person005/4.txt    \n",
            "  inflating: Data/Person005/5.txt    \n",
            "  inflating: Data/Person006/1.txt    \n",
            "  inflating: Data/Person006/2.txt    \n",
            "  inflating: Data/Person006/3.txt    \n",
            "  inflating: Data/Person006/4.txt    \n",
            "  inflating: Data/Person006/5.txt    \n",
            "  inflating: Data/Person007/1.txt    \n",
            "  inflating: Data/Person007/2.txt    \n",
            "  inflating: Data/Person007/3.txt    \n",
            "  inflating: Data/Person007/4.txt    \n",
            "  inflating: Data/Person007/5.txt    \n",
            "  inflating: Data/Person008/1.txt    \n",
            "  inflating: Data/Person008/2.txt    \n",
            "  inflating: Data/Person008/3.txt    \n",
            "  inflating: Data/Person008/4.txt    \n",
            "  inflating: Data/Person008/5.txt    \n",
            "  inflating: Data/Person009/1.txt    \n",
            "  inflating: Data/Person009/2.txt    \n",
            "  inflating: Data/Person009/3.txt    \n",
            "  inflating: Data/Person009/4.txt    \n",
            "  inflating: Data/Person009/5.txt    \n",
            "  inflating: Data/Person010/1.txt    \n",
            "  inflating: Data/Person010/2.txt    \n",
            "  inflating: Data/Person010/3.txt    \n",
            "  inflating: Data/Person010/4.txt    \n",
            "  inflating: Data/Person010/5.txt    \n",
            "  inflating: Data/Person011/1.txt    \n",
            "  inflating: Data/Person011/2.txt    \n",
            "  inflating: Data/Person011/3.txt    \n",
            "  inflating: Data/Person011/4.txt    \n",
            "  inflating: Data/Person011/5.txt    \n",
            "  inflating: Data/Person012/1.txt    \n",
            "  inflating: Data/Person012/2.txt    \n",
            "  inflating: Data/Person012/3.txt    \n",
            "  inflating: Data/Person012/4.txt    \n",
            "  inflating: Data/Person012/5.txt    \n",
            "  inflating: Data/Person013/1.txt    \n",
            "  inflating: Data/Person013/2.txt    \n",
            "  inflating: Data/Person013/3.txt    \n",
            "  inflating: Data/Person013/4.txt    \n",
            "  inflating: Data/Person013/5.txt    \n",
            "  inflating: Data/Person014/1.txt    \n",
            "  inflating: Data/Person014/2.txt    \n",
            "  inflating: Data/Person014/3.txt    \n",
            "  inflating: Data/Person014/4.txt    \n",
            "  inflating: Data/Person014/5.txt    \n",
            "  inflating: Data/Person015/1.txt    \n",
            "  inflating: Data/Person015/2.txt    \n",
            "  inflating: Data/Person015/3.txt    \n",
            "  inflating: Data/Person016/1.txt    \n",
            "  inflating: Data/Person016/2.txt    \n",
            "  inflating: Data/Person016/3.txt    \n",
            "  inflating: Data/Person016/4.txt    \n",
            "  inflating: Data/Person016/5.txt    \n",
            "  inflating: Data/Person017/1.txt    \n",
            "  inflating: Data/Person017/2.txt    \n",
            "  inflating: Data/Person017/3.txt    \n",
            "  inflating: Data/Person017/4.txt    \n",
            "  inflating: Data/Person017/5.txt    \n",
            "  inflating: Data/Person018/1.txt    \n",
            "  inflating: Data/Person018/2.txt    \n",
            "  inflating: Data/Person018/3.txt    \n",
            "  inflating: Data/Person018/4.txt    \n",
            "  inflating: Data/Person018/5.txt    \n",
            "  inflating: Data/Person019/1.txt    \n",
            "  inflating: Data/Person019/2.txt    \n",
            "  inflating: Data/Person019/3.txt    \n",
            "  inflating: Data/Person019/4.txt    \n",
            "  inflating: Data/Person019/5.txt    \n",
            "  inflating: Data/Person020/1.txt    \n",
            "  inflating: Data/Person020/2.txt    \n",
            "  inflating: Data/Person020/3.txt    \n",
            "  inflating: Data/Person020/4.txt    \n",
            "  inflating: Data/Person020/5.txt    \n",
            "  inflating: Data/Person021/1.txt    \n",
            "  inflating: Data/Person021/2.txt    \n",
            "  inflating: Data/Person021/3.txt    \n",
            "  inflating: Data/Person021/4.txt    \n",
            "  inflating: Data/Person021/5.txt    \n",
            "  inflating: Data/Person022/1.txt    \n",
            "  inflating: Data/Person022/2.txt    \n",
            "  inflating: Data/Person022/3.txt    \n",
            "  inflating: Data/Person022/4.txt    \n",
            "  inflating: Data/Person022/5.txt    \n",
            "  inflating: Data/Person023/1.txt    \n",
            "  inflating: Data/Person023/2.txt    \n",
            "  inflating: Data/Person023/3.txt    \n",
            "  inflating: Data/Person023/4.txt    \n",
            "  inflating: Data/Person023/5.txt    \n",
            "  inflating: Data/Person024/1.txt    \n",
            "  inflating: Data/Person024/2.txt    \n",
            "  inflating: Data/Person024/3.txt    \n",
            "  inflating: Data/Person024/4.txt    \n",
            "  inflating: Data/Person024/5.txt    \n",
            "  inflating: Data/Person025/1.txt    \n",
            "  inflating: Data/Person025/2.txt    \n",
            "  inflating: Data/Person025/3.txt    \n",
            "  inflating: Data/Person025/4.txt    \n",
            "  inflating: Data/Person025/5.txt    \n",
            "  inflating: Data/Person026/1.txt    \n",
            "  inflating: Data/Person026/2.txt    \n",
            "  inflating: Data/Person026/3.txt    \n",
            "  inflating: Data/Person026/4.txt    \n",
            "  inflating: Data/Person026/5.txt    \n",
            "  inflating: Data/Person027/1.txt    \n",
            "  inflating: Data/Person027/2.txt    \n",
            "  inflating: Data/Person027/3.txt    \n",
            "  inflating: Data/Person027/4.txt    \n",
            "  inflating: Data/Person027/5.txt    \n",
            "  inflating: Data/Person028/1.txt    \n",
            "  inflating: Data/Person028/2.txt    \n",
            "  inflating: Data/Person028/3.txt    \n",
            "  inflating: Data/Person028/4.txt    \n",
            "  inflating: Data/Person028/5.txt    \n",
            "  inflating: Data/Person029/1.txt    \n",
            "  inflating: Data/Person029/2.txt    \n",
            "  inflating: Data/Person029/3.txt    \n",
            "  inflating: Data/Person029/4.txt    \n",
            "  inflating: Data/Person029/5.txt    \n",
            "  inflating: Data/Person030/1.txt    \n",
            "  inflating: Data/Person030/2.txt    \n",
            "  inflating: Data/Person030/3.txt    \n",
            "  inflating: Data/Person030/4.txt    \n",
            "  inflating: Data/Person030/5.txt    \n",
            "  inflating: Data/Person031/1.txt    \n",
            "  inflating: Data/Person031/2.txt    \n",
            "  inflating: Data/Person031/3.txt    \n",
            "  inflating: Data/Person031/4.txt    \n",
            "  inflating: Data/Person031/5.txt    \n",
            "  inflating: Data/Person032/1.txt    \n",
            "  inflating: Data/Person032/2.txt    \n",
            "  inflating: Data/Person032/3.txt    \n",
            "  inflating: Data/Person032/4.txt    \n",
            "  inflating: Data/Person032/5.txt    \n",
            "  inflating: Data/Person033/1.txt    \n",
            "  inflating: Data/Person033/2.txt    \n",
            "  inflating: Data/Person033/3.txt    \n",
            "  inflating: Data/Person033/4.txt    \n",
            "  inflating: Data/Person033/5.txt    \n",
            "  inflating: Data/Person034/1.txt    \n",
            "  inflating: Data/Person034/2.txt    \n",
            "  inflating: Data/Person034/3.txt    \n",
            "  inflating: Data/Person034/4.txt    \n",
            "  inflating: Data/Person034/5.txt    \n",
            "  inflating: Data/Person034/6.txt    \n",
            "  inflating: Data/Person035/1.txt    \n",
            "  inflating: Data/Person035/2.txt    \n",
            "  inflating: Data/Person035/3.txt    \n",
            "  inflating: Data/Person035/4.txt    \n",
            "  inflating: Data/Person035/5.txt    \n",
            "  inflating: Data/Person036/1.txt    \n",
            "  inflating: Data/Person036/2.txt    \n",
            "  inflating: Data/Person036/3.txt    \n",
            "  inflating: Data/Person036/4.txt    \n",
            "  inflating: Data/Person036/5.txt    \n",
            "  inflating: Data/Person036/6.txt    \n",
            "  inflating: Data/Person037/1.txt    \n",
            "  inflating: Data/Person037/2.txt    \n",
            "  inflating: Data/Person037/3.txt    \n",
            "  inflating: Data/Person037/4.txt    \n",
            "  inflating: Data/Person037/5.txt    \n",
            "  inflating: Data/Person038/1.txt    \n",
            "  inflating: Data/Person038/2.txt    \n",
            "  inflating: Data/Person038/3.txt    \n",
            "  inflating: Data/Person038/4.txt    \n",
            "  inflating: Data/Person038/5.txt    \n",
            "  inflating: Data/Person039/1.txt    \n",
            "  inflating: Data/Person039/2.txt    \n",
            "  inflating: Data/Person039/3.txt    \n",
            "  inflating: Data/Person039/4.txt    \n",
            "  inflating: Data/Person039/5.txt    \n",
            "  inflating: Data/Person040/1.txt    \n",
            "  inflating: Data/Person040/2.txt    \n",
            "  inflating: Data/Person040/3.txt    \n",
            "  inflating: Data/Person040/4.txt    \n",
            "  inflating: Data/Person040/5.txt    \n",
            "  inflating: Data/Person041/1.txt    \n",
            "  inflating: Data/Person041/2.txt    \n",
            "  inflating: Data/Person041/3.txt    \n",
            "  inflating: Data/Person041/4.txt    \n",
            "  inflating: Data/Person041/5.txt    \n",
            "  inflating: Data/Person042/1.txt    \n",
            "  inflating: Data/Person042/2.txt    \n",
            "  inflating: Data/Person042/3.txt    \n",
            "  inflating: Data/Person042/4.txt    \n",
            "  inflating: Data/Person042/5.txt    \n",
            "  inflating: Data/Person043/1.txt    \n",
            "  inflating: Data/Person043/2.txt    \n",
            "  inflating: Data/Person043/3.txt    \n",
            "  inflating: Data/Person043/4.txt    \n",
            "  inflating: Data/Person043/5.txt    \n",
            "  inflating: Data/Person044/1.txt    \n",
            "  inflating: Data/Person044/2.txt    \n",
            "  inflating: Data/Person044/3.txt    \n",
            "  inflating: Data/Person044/4.txt    \n",
            "  inflating: Data/Person044/5.txt    \n",
            "  inflating: Data/Person045/1.txt    \n",
            "  inflating: Data/Person045/2.txt    \n",
            "  inflating: Data/Person045/3.txt    \n",
            "  inflating: Data/Person045/4.txt    \n",
            "  inflating: Data/Person045/5.txt    \n",
            "  inflating: Data/Person046/1.txt    \n",
            "  inflating: Data/Person046/2.txt    \n",
            "  inflating: Data/Person046/3.txt    \n",
            "  inflating: Data/Person046/4.txt    \n",
            "  inflating: Data/Person046/5.txt    \n",
            "  inflating: Data/Person047/1.txt    \n",
            "  inflating: Data/Person047/2.txt    \n",
            "  inflating: Data/Person047/3.txt    \n",
            "  inflating: Data/Person047/4.txt    \n",
            "  inflating: Data/Person047/5.txt    \n",
            "  inflating: Data/Person048/1.txt    \n",
            "  inflating: Data/Person048/2.txt    \n",
            "  inflating: Data/Person048/3.txt    \n",
            "  inflating: Data/Person048/4.txt    \n",
            "  inflating: Data/Person048/5.txt    \n",
            "  inflating: Data/Person049/1.txt    \n",
            "  inflating: Data/Person049/2.txt    \n",
            "  inflating: Data/Person049/3.txt    \n",
            "  inflating: Data/Person049/4.txt    \n",
            "  inflating: Data/Person049/5.txt    \n",
            "  inflating: Data/Person050/1.txt    \n",
            "  inflating: Data/Person050/2.txt    \n",
            "  inflating: Data/Person050/3.txt    \n",
            "  inflating: Data/Person050/4.txt    \n",
            "  inflating: Data/Person050/5.txt    \n",
            "  inflating: Data/Person051/1.txt    \n",
            "  inflating: Data/Person051/2.txt    \n",
            "  inflating: Data/Person051/3.txt    \n",
            "  inflating: Data/Person051/4.txt    \n",
            "  inflating: Data/Person051/5.txt    \n",
            "  inflating: Data/Person052/1.txt    \n",
            "  inflating: Data/Person052/2.txt    \n",
            "  inflating: Data/Person052/3.txt    \n",
            "  inflating: Data/Person052/4.txt    \n",
            "  inflating: Data/Person052/5.txt    \n",
            "  inflating: Data/Person052/6.txt    \n",
            "  inflating: Data/Person053/1.txt    \n",
            "  inflating: Data/Person053/2.txt    \n",
            "  inflating: Data/Person053/3.txt    \n",
            "  inflating: Data/Person053/4.txt    \n",
            "  inflating: Data/Person053/5.txt    \n",
            "  inflating: Data/Person053/6.txt    \n",
            "  inflating: Data/Person054/1.txt    \n",
            "  inflating: Data/Person054/2.txt    \n",
            "  inflating: Data/Person054/3.txt    \n",
            "  inflating: Data/Person054/4.txt    \n",
            "  inflating: Data/Person054/5.txt    \n",
            "  inflating: Data/Person055/1.txt    \n",
            "  inflating: Data/Person055/2.txt    \n",
            "  inflating: Data/Person055/3.txt    \n",
            "  inflating: Data/Person055/4.txt    \n",
            "  inflating: Data/Person055/5.txt    \n",
            "  inflating: Data/Person056/1.txt    \n",
            "  inflating: Data/Person056/2.txt    \n",
            "  inflating: Data/Person056/3.txt    \n",
            "  inflating: Data/Person056/4.txt    \n",
            "  inflating: Data/Person056/5.txt    \n",
            "  inflating: Data/Person057/1.txt    \n",
            "  inflating: Data/Person057/2.txt    \n",
            "  inflating: Data/Person057/3.txt    \n",
            "  inflating: Data/Person057/4.txt    \n",
            "  inflating: Data/Person057/5.txt    \n",
            "  inflating: Data/Person058/1.txt    \n",
            "  inflating: Data/Person058/2.txt    \n",
            "  inflating: Data/Person058/3.txt    \n",
            "  inflating: Data/Person058/4.txt    \n",
            "  inflating: Data/Person058/5.txt    \n",
            "  inflating: Data/Person059/1.txt    \n",
            "  inflating: Data/Person059/2.txt    \n",
            "  inflating: Data/Person059/3.txt    \n",
            "  inflating: Data/Person059/4.txt    \n",
            "  inflating: Data/Person059/5.txt    \n",
            "  inflating: Data/Person060/1.txt    \n",
            "  inflating: Data/Person060/2.txt    \n",
            "  inflating: Data/Person060/3.txt    \n",
            "  inflating: Data/Person060/4.txt    \n",
            "  inflating: Data/Person060/5.txt    \n",
            "  inflating: Data/Person061/1.txt    \n",
            "  inflating: Data/Person061/2.txt    \n",
            "  inflating: Data/Person061/3.txt    \n",
            "  inflating: Data/Person061/4.txt    \n",
            "  inflating: Data/Person061/5.txt    \n",
            "  inflating: Data/Person062/1.txt    \n",
            "  inflating: Data/Person062/2.txt    \n",
            "  inflating: Data/Person062/3.txt    \n",
            "  inflating: Data/Person062/4.txt    \n",
            "  inflating: Data/Person062/5.txt    \n",
            "  inflating: Data/Person063/1.txt    \n",
            "  inflating: Data/Person063/2.txt    \n",
            "  inflating: Data/Person063/3.txt    \n",
            "  inflating: Data/Person063/4.txt    \n",
            "  inflating: Data/Person063/5.txt    \n",
            "  inflating: Data/Person064/1.txt    \n",
            "  inflating: Data/Person064/2.txt    \n",
            "  inflating: Data/Person064/3.txt    \n",
            "  inflating: Data/Person064/4.txt    \n",
            "  inflating: Data/Person064/5.txt    \n",
            "  inflating: Data/Person065/1.txt    \n",
            "  inflating: Data/Person065/2.txt    \n",
            "  inflating: Data/Person065/3.txt    \n",
            "  inflating: Data/Person065/4.txt    \n",
            "  inflating: Data/Person065/5.txt    \n",
            "  inflating: Data/Person066/1.txt    \n",
            "  inflating: Data/Person066/2.txt    \n",
            "  inflating: Data/Person066/3.txt    \n",
            "  inflating: Data/Person066/4.txt    \n",
            "  inflating: Data/Person066/5.txt    \n",
            "  inflating: Data/Person067/1.txt    \n",
            "  inflating: Data/Person067/2.txt    \n",
            "  inflating: Data/Person067/3.txt    \n",
            "  inflating: Data/Person067/4.txt    \n",
            "  inflating: Data/Person067/5.txt    \n",
            "  inflating: Data/Person068/1.txt    \n",
            "  inflating: Data/Person068/2.txt    \n",
            "  inflating: Data/Person068/3.txt    \n",
            "  inflating: Data/Person068/4.txt    \n",
            "  inflating: Data/Person068/5.txt    \n",
            "  inflating: Data/Person069/1.txt    \n",
            "  inflating: Data/Person069/2.txt    \n",
            "  inflating: Data/Person069/3.txt    \n",
            "  inflating: Data/Person069/4.txt    \n",
            "  inflating: Data/Person069/5.txt    \n",
            "  inflating: Data/Person070/1.txt    \n",
            "  inflating: Data/Person070/2.txt    \n",
            "  inflating: Data/Person070/3.txt    \n",
            "  inflating: Data/Person070/4.txt    \n",
            "  inflating: Data/Person070/5.txt    \n",
            "  inflating: Data/Person071/1.txt    \n",
            "  inflating: Data/Person071/2.txt    \n",
            "  inflating: Data/Person071/3.txt    \n",
            "  inflating: Data/Person071/4.txt    \n",
            "  inflating: Data/Person071/5.txt    \n",
            "  inflating: Data/Person072/1.txt    \n",
            "  inflating: Data/Person072/2.txt    \n",
            "  inflating: Data/Person072/3.txt    \n",
            "  inflating: Data/Person072/4.txt    \n",
            "  inflating: Data/Person072/5.txt    \n",
            "  inflating: Data/Person073/1.txt    \n",
            "  inflating: Data/Person073/2.txt    \n",
            "  inflating: Data/Person073/3.txt    \n",
            "  inflating: Data/Person073/4.txt    \n",
            "  inflating: Data/Person073/5.txt    \n",
            "  inflating: Data/Person074/1.txt    \n",
            "  inflating: Data/Person074/2.txt    \n",
            "  inflating: Data/Person074/3.txt    \n",
            "  inflating: Data/Person074/4.txt    \n",
            "  inflating: Data/Person074/5.txt    \n",
            "  inflating: Data/Person074/6.txt    \n",
            "  inflating: Data/Person075/1.txt    \n",
            "  inflating: Data/Person075/2.txt    \n",
            "  inflating: Data/Person075/3.txt    \n",
            "  inflating: Data/Person075/4.txt    \n",
            "  inflating: Data/Person075/5.txt    \n",
            "  inflating: Data/Person076/1.txt    \n",
            "  inflating: Data/Person076/2.txt    \n",
            "  inflating: Data/Person076/3.txt    \n",
            "  inflating: Data/Person076/4.txt    \n",
            "  inflating: Data/Person076/5.txt    \n",
            "  inflating: Data/Person077/1.txt    \n",
            "  inflating: Data/Person077/2.txt    \n",
            "  inflating: Data/Person077/3.txt    \n",
            "  inflating: Data/Person077/4.txt    \n",
            "  inflating: Data/Person077/5.txt    \n",
            "  inflating: Data/Person078/1.txt    \n",
            "  inflating: Data/Person078/2.txt    \n",
            "  inflating: Data/Person078/3.txt    \n",
            "  inflating: Data/Person078/4.txt    \n",
            "  inflating: Data/Person078/5.txt    \n",
            "  inflating: Data/Person079/1.txt    \n",
            "  inflating: Data/Person079/2.txt    \n",
            "  inflating: Data/Person079/3.txt    \n",
            "  inflating: Data/Person079/4.txt    \n",
            "  inflating: Data/Person079/5.txt    \n",
            "  inflating: Data/Person080/1.txt    \n",
            "  inflating: Data/Person080/2.txt    \n",
            "  inflating: Data/Person080/3.txt    \n",
            "  inflating: Data/Person080/4.txt    \n",
            "  inflating: Data/Person080/5.txt    \n",
            "  inflating: Data/Person081/1.txt    \n",
            "  inflating: Data/Person081/2.txt    \n",
            "  inflating: Data/Person081/3.txt    \n",
            "  inflating: Data/Person081/4.txt    \n",
            "  inflating: Data/Person081/5.txt    \n",
            "  inflating: Data/Person082/1.txt    \n",
            "  inflating: Data/Person082/2.txt    \n",
            "  inflating: Data/Person082/3.txt    \n",
            "  inflating: Data/Person082/4.txt    \n",
            "  inflating: Data/Person082/5.txt    \n",
            "  inflating: Data/Person083/1.txt    \n",
            "  inflating: Data/Person083/2.txt    \n",
            "  inflating: Data/Person083/3.txt    \n",
            "  inflating: Data/Person083/4.txt    \n",
            "  inflating: Data/Person083/5.txt    \n",
            "  inflating: Data/Person084/1.txt    \n",
            "  inflating: Data/Person084/2.txt    \n",
            "  inflating: Data/Person084/3.txt    \n",
            "  inflating: Data/Person084/4.txt    \n",
            "  inflating: Data/Person084/5.txt    \n",
            "  inflating: Data/Person085/1.txt    \n",
            "  inflating: Data/Person085/2.txt    \n",
            "  inflating: Data/Person085/3.txt    \n",
            "  inflating: Data/Person085/4.txt    \n",
            "  inflating: Data/Person085/5.txt    \n",
            "  inflating: Data/Person086/1.txt    \n",
            "  inflating: Data/Person086/2.txt    \n",
            "  inflating: Data/Person086/3.txt    \n",
            "  inflating: Data/Person086/4.txt    \n",
            "  inflating: Data/Person086/5.txt    \n",
            "  inflating: Data/Person087/1.txt    \n",
            "  inflating: Data/Person087/2.txt    \n",
            "  inflating: Data/Person087/3.txt    \n",
            "  inflating: Data/Person087/4.txt    \n",
            "  inflating: Data/Person087/5.txt    \n",
            "  inflating: Data/Person088/1.txt    \n",
            "  inflating: Data/Person088/2.txt    \n",
            "  inflating: Data/Person088/3.txt    \n",
            "  inflating: Data/Person088/4.txt    \n",
            "  inflating: Data/Person088/5.txt    \n",
            "  inflating: Data/Person089/1.txt    \n",
            "  inflating: Data/Person089/2.txt    \n",
            "  inflating: Data/Person089/3.txt    \n",
            "  inflating: Data/Person089/4.txt    \n",
            "  inflating: Data/Person089/5.txt    \n",
            "  inflating: Data/Person090/1.txt    \n",
            "  inflating: Data/Person090/2.txt    \n",
            "  inflating: Data/Person090/3.txt    \n",
            "  inflating: Data/Person090/4.txt    \n",
            "  inflating: Data/Person090/5.txt    \n",
            "  inflating: Data/Person091/1.txt    \n",
            "  inflating: Data/Person091/2.txt    \n",
            "  inflating: Data/Person091/3.txt    \n",
            "  inflating: Data/Person091/4.txt    \n",
            "  inflating: Data/Person091/5.txt    \n",
            "  inflating: Data/Person092/1.txt    \n",
            "  inflating: Data/Person092/2.txt    \n",
            "  inflating: Data/Person092/3.txt    \n",
            "  inflating: Data/Person092/4.txt    \n",
            "  inflating: Data/Person092/5.txt    \n",
            "  inflating: Data/Person093/1.txt    \n",
            "  inflating: Data/Person093/2.txt    \n",
            "  inflating: Data/Person093/3.txt    \n",
            "  inflating: Data/Person093/4.txt    \n",
            "  inflating: Data/Person093/5.txt    \n",
            "  inflating: Data/Person094/1.txt    \n",
            "  inflating: Data/Person094/2.txt    \n",
            "  inflating: Data/Person094/3.txt    \n",
            "  inflating: Data/Person094/4.txt    \n",
            "  inflating: Data/Person094/5.txt    \n",
            "  inflating: Data/Person095/1.txt    \n",
            "  inflating: Data/Person095/2.txt    \n",
            "  inflating: Data/Person095/3.txt    \n",
            "  inflating: Data/Person095/4.txt    \n",
            "  inflating: Data/Person095/5.txt    \n",
            "  inflating: Data/Person096/1.txt    \n",
            "  inflating: Data/Person096/2.txt    \n",
            "  inflating: Data/Person096/3.txt    \n",
            "  inflating: Data/Person096/4.txt    \n",
            "  inflating: Data/Person096/5.txt    \n",
            "  inflating: Data/Person096/6.txt    \n",
            "  inflating: Data/Person097/1.txt    \n",
            "  inflating: Data/Person097/2.txt    \n",
            "  inflating: Data/Person097/3.txt    \n",
            "  inflating: Data/Person097/4.txt    \n",
            "  inflating: Data/Person097/5.txt    \n",
            "  inflating: Data/Person098/1.txt    \n",
            "  inflating: Data/Person098/2.txt    \n",
            "  inflating: Data/Person098/3.txt    \n",
            "  inflating: Data/Person098/4.txt    \n",
            "  inflating: Data/Person098/5.txt    \n",
            "  inflating: Data/Person099/1.txt    \n",
            "  inflating: Data/Person099/2.txt    \n",
            "  inflating: Data/Person099/3.txt    \n",
            "  inflating: Data/Person099/4.txt    \n",
            "  inflating: Data/Person099/5.txt    \n",
            "  inflating: Data/Person100/1.txt    \n",
            "  inflating: Data/Person100/2.txt    \n",
            "  inflating: Data/Person100/3.txt    \n",
            "  inflating: Data/Person100/4.txt    \n",
            "  inflating: Data/Person100/5.txt    \n",
            "  inflating: Data/Person101/1.txt    \n",
            "  inflating: Data/Person101/2.txt    \n",
            "  inflating: Data/Person101/3.txt    \n",
            "  inflating: Data/Person101/4.txt    \n",
            "  inflating: Data/Person101/5.txt    \n",
            "  inflating: Data/Person102/1.txt    \n",
            "  inflating: Data/Person102/2.txt    \n",
            "  inflating: Data/Person102/3.txt    \n",
            "  inflating: Data/Person102/4.txt    \n",
            "  inflating: Data/Person102/5.txt    \n",
            "  inflating: Data/Person103/1.txt    \n",
            "  inflating: Data/Person103/2.txt    \n",
            "  inflating: Data/Person103/3.txt    \n",
            "  inflating: Data/Person103/4.txt    \n",
            "  inflating: Data/Person103/5.txt    \n",
            "  inflating: Data/Person104/1.txt    \n",
            "  inflating: Data/Person104/2.txt    \n",
            "  inflating: Data/Person104/3.txt    \n",
            "  inflating: Data/Person104/4.txt    \n",
            "  inflating: Data/Person104/5.txt    \n",
            "  inflating: Data/Person105/1.txt    \n",
            "  inflating: Data/Person105/2.txt    \n",
            "  inflating: Data/Person105/3.txt    \n",
            "  inflating: Data/Person105/4.txt    \n",
            "  inflating: Data/Person105/5.txt    \n",
            "  inflating: Data/Person106/1.txt    \n",
            "  inflating: Data/Person106/2.txt    \n",
            "  inflating: Data/Person106/3.txt    \n",
            "  inflating: Data/Person106/4.txt    \n",
            "  inflating: Data/Person106/5.txt    \n",
            "  inflating: Data/Person107/1.txt    \n",
            "  inflating: Data/Person107/2.txt    \n",
            "  inflating: Data/Person107/3.txt    \n",
            "  inflating: Data/Person107/4.txt    \n",
            "  inflating: Data/Person107/5.txt    \n",
            "  inflating: Data/Person108/1.txt    \n",
            "  inflating: Data/Person108/2.txt    \n",
            "  inflating: Data/Person108/3.txt    \n",
            "  inflating: Data/Person108/4.txt    \n",
            "  inflating: Data/Person108/5.txt    \n",
            "  inflating: Data/Person109/1.txt    \n",
            "  inflating: Data/Person109/2.txt    \n",
            "  inflating: Data/Person109/3.txt    \n",
            "  inflating: Data/Person109/4.txt    \n",
            "  inflating: Data/Person109/5.txt    \n",
            "  inflating: Data/Person110/1.txt    \n",
            "  inflating: Data/Person110/2.txt    \n",
            "  inflating: Data/Person110/3.txt    \n",
            "  inflating: Data/Person110/4.txt    \n",
            "  inflating: Data/Person110/5.txt    \n",
            "  inflating: Data/Person111/1.txt    \n",
            "  inflating: Data/Person111/2.txt    \n",
            "  inflating: Data/Person111/3.txt    \n",
            "  inflating: Data/Person111/4.txt    \n",
            "  inflating: Data/Person111/5.txt    \n",
            "  inflating: Data/Person112/1.txt    \n",
            "  inflating: Data/Person112/2.txt    \n",
            "  inflating: Data/Person112/3.txt    \n",
            "  inflating: Data/Person112/4.txt    \n",
            "  inflating: Data/Person112/5.txt    \n",
            "  inflating: Data/Person113/1.txt    \n",
            "  inflating: Data/Person113/2.txt    \n",
            "  inflating: Data/Person113/3.txt    \n",
            "  inflating: Data/Person113/4.txt    \n",
            "  inflating: Data/Person113/5.txt    \n",
            "  inflating: Data/Person114/1.txt    \n",
            "  inflating: Data/Person114/2.txt    \n",
            "  inflating: Data/Person114/3.txt    \n",
            "  inflating: Data/Person114/4.txt    \n",
            "  inflating: Data/Person114/5.txt    \n",
            "  inflating: Data/Person115/1.txt    \n",
            "  inflating: Data/Person115/2.txt    \n",
            "  inflating: Data/Person115/3.txt    \n",
            "  inflating: Data/Person115/4.txt    \n",
            "  inflating: Data/Person115/5.txt    \n",
            "  inflating: Data/Person116/1.txt    \n",
            "  inflating: Data/Person116/2.txt    \n",
            "  inflating: Data/Person116/3.txt    \n",
            "  inflating: Data/Person116/4.txt    \n",
            "  inflating: Data/Person116/5.txt    \n",
            "  inflating: Data/Person117/1.txt    \n",
            "  inflating: Data/Person117/2.txt    \n",
            "  inflating: Data/Person117/3.txt    \n",
            "  inflating: Data/Person117/4.txt    \n",
            "  inflating: Data/Person117/5.txt    \n",
            "  inflating: Data/Person118/1.txt    \n",
            "  inflating: Data/Person118/2.txt    \n",
            "  inflating: Data/Person118/3.txt    \n",
            "  inflating: Data/Person118/4.txt    \n",
            "  inflating: Data/Person118/5.txt    \n",
            "  inflating: Data/Person119/1.txt    \n",
            "  inflating: Data/Person119/2.txt    \n",
            "  inflating: Data/Person119/3.txt    \n",
            "  inflating: Data/Person119/4.txt    \n",
            "  inflating: Data/Person119/5.txt    \n",
            "  inflating: Data/Person120/1.txt    \n",
            "  inflating: Data/Person120/2.txt    \n",
            "  inflating: Data/Person120/3.txt    \n",
            "  inflating: Data/Person120/4.txt    \n",
            "  inflating: Data/Person120/5.txt    \n",
            "  inflating: Data/Person121/1.txt    \n",
            "  inflating: Data/Person121/2.txt    \n",
            "  inflating: Data/Person121/3.txt    \n",
            "  inflating: Data/Person121/4.txt    \n",
            "  inflating: Data/Person121/5.txt    \n",
            "  inflating: Data/Person122/1.txt    \n",
            "  inflating: Data/Person122/2.txt    \n",
            "  inflating: Data/Person122/3.txt    \n",
            "  inflating: Data/Person122/4.txt    \n",
            "  inflating: Data/Person122/5.txt    \n",
            "  inflating: Data/Person123/1.txt    \n",
            "  inflating: Data/Person123/2.txt    \n",
            "  inflating: Data/Person123/3.txt    \n",
            "  inflating: Data/Person123/4.txt    \n",
            "  inflating: Data/Person123/5.txt    \n",
            "  inflating: Data/Person124/1.txt    \n",
            "  inflating: Data/Person124/2.txt    \n",
            "  inflating: Data/Person124/3.txt    \n",
            "  inflating: Data/Person124/4.txt    \n",
            "  inflating: Data/Person124/5.txt    \n",
            "  inflating: Data/Person125/1.txt    \n",
            "  inflating: Data/Person125/2.txt    \n",
            "  inflating: Data/Person125/3.txt    \n",
            "  inflating: Data/Person125/4.txt    \n",
            "  inflating: Data/Person125/5.txt    \n",
            "  inflating: Data/Person126/1.txt    \n",
            "  inflating: Data/Person126/2.txt    \n",
            "  inflating: Data/Person126/3.txt    \n",
            "  inflating: Data/Person126/4.txt    \n",
            "  inflating: Data/Person126/5.txt    \n",
            "  inflating: Data/Person127/1.txt    \n",
            "  inflating: Data/Person127/2.txt    \n",
            "  inflating: Data/Person127/3.txt    \n",
            "  inflating: Data/Person127/4.txt    \n",
            "  inflating: Data/Person127/5.txt    \n",
            "  inflating: Data/Person128/1.txt    \n",
            "  inflating: Data/Person128/2.txt    \n",
            "  inflating: Data/Person128/3.txt    \n",
            "  inflating: Data/Person128/4.txt    \n",
            "  inflating: Data/Person128/5.txt    \n",
            "  inflating: Data/Person129/1.txt    \n",
            "  inflating: Data/Person129/2.txt    \n",
            "  inflating: Data/Person129/3.txt    \n",
            "  inflating: Data/Person129/4.txt    \n",
            "  inflating: Data/Person129/5.txt    \n",
            "  inflating: Data/Person130/1.txt    \n",
            "  inflating: Data/Person130/2.txt    \n",
            "  inflating: Data/Person130/3.txt    \n",
            "  inflating: Data/Person130/4.txt    \n",
            "  inflating: Data/Person130/5.txt    \n",
            "  inflating: Data/Person131/1.txt    \n",
            "  inflating: Data/Person131/2.txt    \n",
            "  inflating: Data/Person131/3.txt    \n",
            "  inflating: Data/Person131/4.txt    \n",
            "  inflating: Data/Person131/5.txt    \n",
            "  inflating: Data/Person132/1.txt    \n",
            "  inflating: Data/Person132/2.txt    \n",
            "  inflating: Data/Person132/3.txt    \n",
            "  inflating: Data/Person132/4.txt    \n",
            "  inflating: Data/Person132/5.txt    \n",
            "  inflating: Data/Person133/1.txt    \n",
            "  inflating: Data/Person133/2.txt    \n",
            "  inflating: Data/Person133/3.txt    \n",
            "  inflating: Data/Person133/4.txt    \n",
            "  inflating: Data/Person133/5.txt    \n",
            "  inflating: Data/Person134/1.txt    \n",
            "  inflating: Data/Person134/2.txt    \n",
            "  inflating: Data/Person134/3.txt    \n",
            "  inflating: Data/Person134/4.txt    \n",
            "  inflating: Data/Person134/5.txt    \n",
            "  inflating: Data/Person135/1.txt    \n",
            "  inflating: Data/Person135/2.txt    \n",
            "  inflating: Data/Person135/3.txt    \n",
            "  inflating: Data/Person135/4.txt    \n",
            "  inflating: Data/Person135/5.txt    \n",
            "  inflating: Data/Person136/1.txt    \n",
            "  inflating: Data/Person136/2.txt    \n",
            "  inflating: Data/Person136/3.txt    \n",
            "  inflating: Data/Person136/4.txt    \n",
            "  inflating: Data/Person136/5.txt    \n",
            "  inflating: Data/Person137/1.txt    \n",
            "  inflating: Data/Person137/2.txt    \n",
            "  inflating: Data/Person137/3.txt    \n",
            "  inflating: Data/Person137/4.txt    \n",
            "  inflating: Data/Person137/5.txt    \n",
            "  inflating: Data/Person138/1.txt    \n",
            "  inflating: Data/Person138/2.txt    \n",
            "  inflating: Data/Person138/3.txt    \n",
            "  inflating: Data/Person138/4.txt    \n",
            "  inflating: Data/Person138/5.txt    \n",
            "  inflating: Data/Person139/1.txt    \n",
            "  inflating: Data/Person139/2.txt    \n",
            "  inflating: Data/Person139/3.txt    \n",
            "  inflating: Data/Person139/4.txt    \n",
            "  inflating: Data/Person139/5.txt    \n",
            "  inflating: Data/Person140/1.txt    \n",
            "  inflating: Data/Person140/2.txt    \n",
            "  inflating: Data/Person140/3.txt    \n",
            "  inflating: Data/Person140/4.txt    \n",
            "  inflating: Data/Person140/5.txt    \n",
            "  inflating: Data/Person141/1.txt    \n",
            "  inflating: Data/Person141/2.txt    \n",
            "  inflating: Data/Person141/3.txt    \n",
            "  inflating: Data/Person141/4.txt    \n",
            "  inflating: Data/Person141/5.txt    \n",
            "  inflating: Data/Person142/1.txt    \n",
            "  inflating: Data/Person142/2.txt    \n",
            "  inflating: Data/Person142/3.txt    \n",
            "  inflating: Data/Person142/4.txt    \n",
            "  inflating: Data/Person142/5.txt    \n",
            "  inflating: Data/Person143/1.txt    \n",
            "  inflating: Data/Person143/2.txt    \n",
            "  inflating: Data/Person143/3.txt    \n",
            "  inflating: Data/Person143/4.txt    \n",
            "  inflating: Data/Person143/5.txt    \n",
            "  inflating: Data/Person144/1.txt    \n",
            "  inflating: Data/Person144/2.txt    \n",
            "  inflating: Data/Person144/3.txt    \n",
            "  inflating: Data/Person144/4.txt    \n",
            "  inflating: Data/Person144/5.txt    \n",
            "  inflating: Data/Person145/1.txt    \n",
            "  inflating: Data/Person145/2.txt    \n",
            "  inflating: Data/Person145/3.txt    \n",
            "  inflating: Data/Person145/4.txt    \n",
            "  inflating: Data/Person145/5.txt    \n",
            "  inflating: Data/Person146/1.txt    \n",
            "  inflating: Data/Person146/2.txt    \n",
            "  inflating: Data/Person146/3.txt    \n",
            "  inflating: Data/Person146/4.txt    \n",
            "  inflating: Data/Person146/5.txt    \n",
            "  inflating: Data/Person147/1.txt    \n",
            "  inflating: Data/Person147/2.txt    \n",
            "  inflating: Data/Person147/3.txt    \n",
            "  inflating: Data/Person147/4.txt    \n",
            "  inflating: Data/Person147/5.txt    \n",
            "  inflating: Data/Person148/1.txt    \n",
            "  inflating: Data/Person148/2.txt    \n",
            "  inflating: Data/Person148/3.txt    \n",
            "  inflating: Data/Person148/4.txt    \n",
            "  inflating: Data/Person148/5.txt    \n",
            "  inflating: Data/Person149/1.txt    \n",
            "  inflating: Data/Person149/2.txt    \n",
            "  inflating: Data/Person149/3.txt    \n",
            "  inflating: Data/Person149/4.txt    \n",
            "  inflating: Data/Person149/5.txt    \n",
            "  inflating: Data/Person150/1.txt    \n",
            "  inflating: Data/Person150/2.txt    \n",
            "  inflating: Data/Person150/3.txt    \n",
            "  inflating: Data/Person150/4.txt    \n",
            "  inflating: Data/Person150/5.txt    \n",
            "  inflating: Data/Person151/1.txt    \n",
            "  inflating: Data/Person151/2.txt    \n",
            "  inflating: Data/Person151/3.txt    \n",
            "  inflating: Data/Person151/4.txt    \n",
            "  inflating: Data/Person151/5.txt    \n",
            "  inflating: Data/Person152/1.txt    \n",
            "  inflating: Data/Person152/2.txt    \n",
            "  inflating: Data/Person152/3.txt    \n",
            "  inflating: Data/Person152/4.txt    \n",
            "  inflating: Data/Person152/5.txt    \n",
            "  inflating: Data/Person153/1.txt    \n",
            "  inflating: Data/Person153/2.txt    \n",
            "  inflating: Data/Person153/3.txt    \n",
            "  inflating: Data/Person153/4.txt    \n",
            "  inflating: Data/Person153/5.txt    \n",
            "  inflating: Data/Person154/1.txt    \n",
            "  inflating: Data/Person154/2.txt    \n",
            "  inflating: Data/Person154/3.txt    \n",
            "  inflating: Data/Person154/4.txt    \n",
            "  inflating: Data/Person154/5.txt    \n",
            "  inflating: Data/Person155/1.txt    \n",
            "  inflating: Data/Person155/2.txt    \n",
            "  inflating: Data/Person155/3.txt    \n",
            "  inflating: Data/Person155/4.txt    \n",
            "  inflating: Data/Person155/5.txt    \n",
            "  inflating: Data/Person156/1.txt    \n",
            "  inflating: Data/Person156/2.txt    \n",
            "  inflating: Data/Person156/3.txt    \n",
            "  inflating: Data/Person156/4.txt    \n",
            "  inflating: Data/Person156/5.txt    \n",
            "  inflating: Data/Person157/1.txt    \n",
            "  inflating: Data/Person157/2.txt    \n",
            "  inflating: Data/Person157/3.txt    \n",
            "  inflating: Data/Person157/4.txt    \n",
            "  inflating: Data/Person157/5.txt    \n",
            "  inflating: Data/Person158/1.txt    \n",
            "  inflating: Data/Person158/2.txt    \n",
            "  inflating: Data/Person158/3.txt    \n",
            "  inflating: Data/Person158/4.txt    \n",
            "  inflating: Data/Person159/1.txt    \n",
            "  inflating: Data/Person159/2.txt    \n",
            "  inflating: Data/Person159/3.txt    \n",
            "  inflating: Data/Person159/4.txt    \n",
            "  inflating: Data/Person159/5.txt    \n",
            "  inflating: Data/Person160/1.txt    \n",
            "  inflating: Data/Person160/2.txt    \n",
            "  inflating: Data/Person160/3.txt    \n",
            "  inflating: Data/Person160/4.txt    \n",
            "  inflating: Data/Person160/5.txt    \n",
            "  inflating: Data/Person161/1.txt    \n",
            "  inflating: Data/Person161/2.txt    \n",
            "  inflating: Data/Person161/3.txt    \n",
            "  inflating: Data/Person161/4.txt    \n",
            "  inflating: Data/Person161/5.txt    \n",
            "  inflating: Data/Person162/1.txt    \n",
            "  inflating: Data/Person162/2.txt    \n",
            "  inflating: Data/Person162/3.txt    \n",
            "  inflating: Data/Person162/4.txt    \n",
            "  inflating: Data/Person162/5.txt    \n",
            "  inflating: Data/Person163/1.txt    \n",
            "  inflating: Data/Person163/2.txt    \n",
            "  inflating: Data/Person163/3.txt    \n",
            "  inflating: Data/Person163/4.txt    \n",
            "  inflating: Data/Person163/5.txt    \n",
            "  inflating: Data/Person164/1.txt    \n",
            "  inflating: Data/Person164/2.txt    \n",
            "  inflating: Data/Person164/3.txt    \n",
            "  inflating: Data/Person164/4.txt    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SkeletonDataProcessor:\n",
        "    def __init__(self, data_dir):\n",
        "        \"\"\"\n",
        "        Initialize the skeleton data processor\n",
        "\n",
        "        Args:\n",
        "            data_dir (str): Path to the root directory containing person folders\n",
        "        \"\"\"\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.person_dirs = sorted([d for d in self.data_dir.iterdir() if d.is_dir()])\n",
        "        self.num_joints = 20\n",
        "\n",
        "    def read_skeleton_file(self, file_path):\n",
        "        \"\"\"\n",
        "        Read and parse a single skeleton file\n",
        "\n",
        "        Args:\n",
        "            file_path (Path): Path to the skeleton file\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Array of shape [time_steps, num_joints, 3]\n",
        "        \"\"\"\n",
        "        with open(file_path, 'r') as file:\n",
        "            lines = file.read().strip().split('\\n')\n",
        "\n",
        "        # Count frames\n",
        "        num_frames = len(lines) // self.num_joints\n",
        "\n",
        "        # Initialize array for this sequence\n",
        "        sequence = np.zeros((num_frames, self.num_joints, 3))\n",
        "\n",
        "        # Process each line\n",
        "        for i, line in enumerate(lines):\n",
        "            parts = line.split(';')\n",
        "\n",
        "            frame_idx = i // self.num_joints\n",
        "            joint_idx = i % self.num_joints\n",
        "\n",
        "            # Store x, y, z coordinates\n",
        "            sequence[frame_idx, joint_idx, 0] = float(parts[1])  # x\n",
        "            sequence[frame_idx, joint_idx, 1] = float(parts[2])  # y\n",
        "            sequence[frame_idx, joint_idx, 2] = float(parts[3])  # z\n",
        "\n",
        "        return sequence\n",
        "\n",
        "    def normalize_sequence(self, sequence):\n",
        "        \"\"\"\n",
        "        Normalize a sequence using hip center as origin\n",
        "\n",
        "        Args:\n",
        "            sequence (np.ndarray): Array of shape [time_steps, num_joints, 3]\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Normalized sequence\n",
        "        \"\"\"\n",
        "        # Find hip center index (assuming it's consistent across all sequences)\n",
        "        hip_center_idx = 8  # Update this based on your joint order\n",
        "\n",
        "        # Normalize each frame\n",
        "        normalized_sequence = sequence.copy()\n",
        "        for frame_idx in range(sequence.shape[0]):\n",
        "            # Get hip center coordinates for current frame\n",
        "            hip_center = sequence[frame_idx, hip_center_idx]\n",
        "\n",
        "            # Translate all joints relative to hip center\n",
        "            normalized_sequence[frame_idx] -= hip_center\n",
        "\n",
        "        # Scale to [0, 1] range\n",
        "        min_vals = normalized_sequence.min(axis=(0, 1), keepdims=True)\n",
        "        max_vals = normalized_sequence.max(axis=(0, 1), keepdims=True)\n",
        "        normalized_sequence = (normalized_sequence - min_vals) / (max_vals - min_vals + 1e-7)\n",
        "\n",
        "        return normalized_sequence\n",
        "\n",
        "    def process_all_data(self):\n",
        "        \"\"\"\n",
        "        Process all skeleton data in the dataset\n",
        "\n",
        "        Returns:\n",
        "            list: List of tuples (person_id, sequence_id, normalized_sequence)\n",
        "        \"\"\"\n",
        "        all_sequences = []\n",
        "\n",
        "        for person_dir in self.person_dirs:\n",
        "            person_id = person_dir.name\n",
        "\n",
        "            # Get all txt files for this person\n",
        "            skeleton_files = sorted(person_dir.glob('*.txt'))\n",
        "\n",
        "            for file_path in skeleton_files:\n",
        "                sequence_id = file_path.stem  # Get filename without extension\n",
        "\n",
        "                # Read and normalize sequence\n",
        "                sequence = self.read_skeleton_file(file_path)\n",
        "                normalized_sequence = self.normalize_sequence(sequence)\n",
        "\n",
        "                # Store with metadata\n",
        "                all_sequences.append((person_id, sequence_id, normalized_sequence))\n",
        "\n",
        "        return all_sequences\n",
        "\n"
      ],
      "metadata": {
        "id": "ERFiceublbTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkeletonDataset(Dataset):\n",
        "    def __init__(self, sequences, max_len=None):\n",
        "        \"\"\"\n",
        "        Create a PyTorch dataset from processed sequences\n",
        "\n",
        "        Args:\n",
        "            sequences (list): List of tuples (person_id, sequence_id, normalized_sequence)\n",
        "            max_len (int, optional): Maximum sequence length for padding\n",
        "        \"\"\"\n",
        "        self.sequences = sequences\n",
        "\n",
        "        # Find max sequence length if not provided\n",
        "        if max_len is None:\n",
        "            max_len = max(seq[2].shape[0] for seq in sequences)\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Create person and sequence ID mappings\n",
        "        self.person_ids = sorted(list(set(seq[0] for seq in sequences)))\n",
        "        self.person_to_idx = {pid: i for i, pid in enumerate(self.person_ids)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        person_id, sequence_id, sequence = self.sequences[idx]\n",
        "\n",
        "        # Convert to tensor\n",
        "        sequence_tensor = torch.FloatTensor(sequence)\n",
        "\n",
        "        # Pad sequence if necessary\n",
        "        if sequence_tensor.size(0) < self.max_len:\n",
        "            padding = torch.zeros(self.max_len - sequence_tensor.size(0),\n",
        "                                sequence_tensor.size(1),\n",
        "                                sequence_tensor.size(2))\n",
        "            sequence_tensor = torch.cat([sequence_tensor, padding], dim=0)\n",
        "\n",
        "        # Create attention mask (1 for real data, 0 for padding)\n",
        "        attention_mask = torch.ones(self.max_len)\n",
        "        attention_mask[sequence.shape[0]:] = 0\n",
        "\n",
        "        return {\n",
        "            'person_id': self.person_to_idx[person_id],\n",
        "            'sequence_id': sequence_id,\n",
        "            'sequence': sequence_tensor,\n",
        "            'attention_mask': attention_mask\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "lAbF9INQlhaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_data_loaders(data_dir, batch_size=32, train_split=0.8, val_split=0.1):\n",
        "#     \"\"\"\n",
        "#     Create data loaders for train, validation, and test sets\n",
        "\n",
        "#     Args:\n",
        "#         data_dir (str): Path to data directory\n",
        "#         batch_size (int): Batch size for data loaders\n",
        "#         train_split (float): Proportion of data for training\n",
        "#         val_split (float): Proportion of data for validation\n",
        "\n",
        "#     Returns:\n",
        "#         tuple: Train, validation, and test data loaders\n",
        "#     \"\"\"\n",
        "#     # Process all data\n",
        "#     processor = SkeletonDataProcessor(data_dir)\n",
        "#     all_sequences = processor.process_all_data()\n",
        "\n",
        "#     # Shuffle sequences\n",
        "#     np.random.shuffle(all_sequences)\n",
        "\n",
        "#     # Split data\n",
        "#     n_sequences = len(all_sequences)\n",
        "#     n_train = int(n_sequences * train_split)\n",
        "#     n_val = int(n_sequences * val_split)\n",
        "\n",
        "#     train_sequences = all_sequences[:n_train]\n",
        "#     val_sequences = all_sequences[n_train:n_train + n_val]\n",
        "#     test_sequences = all_sequences[n_train + n_val:]\n",
        "\n",
        "#     # Create datasets\n",
        "#     train_dataset = SkeletonDataset(train_sequences)\n",
        "#     val_dataset = SkeletonDataset(val_sequences, max_len=train_dataset.max_len)\n",
        "#     test_dataset = SkeletonDataset(test_sequences, max_len=train_dataset.max_len)\n",
        "\n",
        "#     # Create data loaders\n",
        "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "#     val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "#     test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "#     return train_loader, val_loader, test_loader\n",
        "\n",
        "def _get_distribution_stats(sequences):\n",
        "    \"\"\"Helper function to calculate distribution statistics\"\"\"\n",
        "    person_counts = {}\n",
        "    for person_id, _, _ in sequences:\n",
        "        person_counts[person_id] = person_counts.get(person_id, 0) + 1\n",
        "\n",
        "    if not person_counts:\n",
        "        return (0, 0, 0)\n",
        "\n",
        "    counts = list(person_counts.values())\n",
        "    return (min(counts), sum(counts)/len(counts), max(counts))\n",
        "\n",
        "def create_data_loaders(data_dir, batch_size=32, train_split=0.8, val_split=0.1):\n",
        "    \"\"\"\n",
        "    Create data loaders for train, validation, and test sets with stratified splitting\n",
        "    ensuring each person has representation in the training set.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): Path to data directory\n",
        "        batch_size (int): Batch size for data loaders\n",
        "        train_split (float): Proportion of data for training\n",
        "        val_split (float): Proportion of data for validation\n",
        "\n",
        "    Returns:\n",
        "        tuple: Train, validation, and test data loaders\n",
        "    \"\"\"\n",
        "    # Process all data\n",
        "    processor = SkeletonDataProcessor(data_dir)\n",
        "    all_sequences = processor.process_all_data()\n",
        "\n",
        "    # Group sequences by person_id\n",
        "    person_sequences = {}\n",
        "    for person_id, seq_id, normalized_seq in all_sequences:\n",
        "        if person_id not in person_sequences:\n",
        "            person_sequences[person_id] = []\n",
        "        person_sequences[person_id].append((person_id, seq_id, normalized_seq))\n",
        "\n",
        "    train_sequences = []\n",
        "    val_sequences = []\n",
        "    test_sequences = []\n",
        "\n",
        "    # For each person, split their sequences\n",
        "    for person_id, sequences in person_sequences.items():\n",
        "        n_sequences = len(sequences)\n",
        "\n",
        "        # Ensure at least one sequence in training\n",
        "        n_train = max(1, int(n_sequences * train_split))\n",
        "        n_val = int(n_sequences * val_split)\n",
        "\n",
        "        # Shuffle sequences for this person\n",
        "        np.random.shuffle(sequences)\n",
        "\n",
        "        # Split sequences for this person\n",
        "        train_sequences.extend(sequences[:n_train])\n",
        "        val_sequences.extend(sequences[n_train:n_train + n_val])\n",
        "        test_sequences.extend(sequences[n_train + n_val:])\n",
        "\n",
        "    # Shuffle the final sets\n",
        "    np.random.shuffle(train_sequences)\n",
        "    np.random.shuffle(val_sequences)\n",
        "    np.random.shuffle(test_sequences)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = SkeletonDataset(train_sequences)\n",
        "    val_dataset = SkeletonDataset(val_sequences, max_len=train_dataset.max_len)\n",
        "    test_dataset = SkeletonDataset(test_sequences, max_len=train_dataset.max_len)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Print distribution statistics\n",
        "    print(\"\\nData distribution statistics:\")\n",
        "    print(f\"Total number of people: {len(person_sequences)}\")\n",
        "    print(f\"Training samples per person (min/avg/max): {_get_distribution_stats(train_sequences)}\")\n",
        "    print(f\"Total samples - Train: {len(train_sequences)}, Val: {len(val_sequences)}, Test: {len(test_sequences)}\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n"
      ],
      "metadata": {
        "id": "crw2H09gloVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "# if __name__ == \"__main__\":\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Directory containing the dataset\n",
        "data_dir = \"/content/Data\"\n",
        "sb = ''\n",
        "try:\n",
        "    # Create data loaders\n",
        "    train_loader, val_loader, test_loader = create_data_loaders(data_dir)\n",
        "\n",
        "    print(f\"Number of training batches: {len(train_loader)}\")\n",
        "    print(f\"Number of validation batches: {len(val_loader)}\")\n",
        "    print(f\"Number of test batches: {len(test_loader)}\")\n",
        "\n",
        "    # Get a sample batch\n",
        "    sample_batch = next(iter(train_loader))\n",
        "    # print(sample_batch)\n",
        "    print(\"\\nSample batch contents:\")\n",
        "    for key, value in sample_batch.items():\n",
        "        if torch.is_tensor(value):\n",
        "            print(f\"{key} shape: {value.shape}\")\n",
        "        else:\n",
        "            print(f\"{key}: {value}\")\n",
        "\n",
        "    # Save dataset statistics\n",
        "    stats = {\n",
        "        'num_training_sequences': len(train_loader.dataset),\n",
        "        'num_validation_sequences': len(val_loader.dataset),\n",
        "        'num_test_sequences': len(test_loader.dataset),\n",
        "        'max_sequence_length': train_loader.dataset.max_len,\n",
        "        'num_joints': 20,\n",
        "        'num_persons': len(train_loader.dataset.person_ids)\n",
        "    }\n",
        "\n",
        "    pd.DataFrame([stats]).to_csv('dataset_statistics.csv', index=False)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error processing dataset: {str(e)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3q7f73kVlsGB",
        "outputId": "3211e92a-d35c-47bd-b6cc-3f361470788e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data distribution statistics:\n",
            "Total number of people: 164\n",
            "Training samples per person (min/avg/max): (2, 3.9695121951219514, 4)\n",
            "Total samples - Train: 651, Val: 0, Test: 171\n",
            "Number of training batches: 21\n",
            "Number of validation batches: 0\n",
            "Number of test batches: 6\n",
            "\n",
            "Sample batch contents:\n",
            "person_id shape: torch.Size([32])\n",
            "sequence_id: ['4', '4', '2', '4', '4', '2', '2', '4', '1', '4', '4', '2', '5', '5', '2', '4', '4', '2', '1', '4', '2', '3', '1', '3', '5', '5', '2', '5', '2', '2', '3', '1']\n",
            "sequence shape: torch.Size([32, 1711, 20, 3])\n",
            "attention_mask shape: torch.Size([32, 1711])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # prompt: print a datapoint\n",
        "\n",
        "# # Access a single data point from the training dataset\n",
        "# data_point = train_dataset[0]\n",
        "\n",
        "# # Print the contents of the data point\n",
        "# print(data_point)\n"
      ],
      "metadata": {
        "id": "DF-05MEaosJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (previous imports remain the same)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "from typing import Dict, Tuple\n",
        "# import wandb  # for logging\n",
        "import random\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "class TemporalAugmenter:\n",
        "    def __init__(\n",
        "        self,\n",
        "        crop_ratio_range=(0.8, 0.9),\n",
        "        mask_ratio_range=(0.1, 0.2),\n",
        "        min_sequence_length=16\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize temporal augmentation parameters\n",
        "\n",
        "        Args:\n",
        "            crop_ratio_range (tuple): Range for random crop ratio\n",
        "            mask_ratio_range (tuple): Range for random masking ratio\n",
        "            min_sequence_length (int): Minimum sequence length after cropping\n",
        "        \"\"\"\n",
        "        self.crop_ratio_range = crop_ratio_range\n",
        "        self.mask_ratio_range = mask_ratio_range\n",
        "        self.min_sequence_length = min_sequence_length\n",
        "\n",
        "    def random_temporal_crop(\n",
        "        self,\n",
        "        sequence: torch.Tensor,\n",
        "        attention_mask: torch.Tensor = None\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Apply random temporal cropping to sequence\"\"\"\n",
        "        seq_length = sequence.size(1)\n",
        "\n",
        "        # Determine crop size\n",
        "        min_ratio, max_ratio = self.crop_ratio_range\n",
        "        crop_ratio = random.uniform(min_ratio, max_ratio)\n",
        "        crop_size = max(int(seq_length * crop_ratio), self.min_sequence_length)\n",
        "\n",
        "        # Random start point\n",
        "        max_start = seq_length - crop_size\n",
        "        start_idx = random.randint(0, max_start)\n",
        "        end_idx = start_idx + crop_size\n",
        "\n",
        "        # Apply crop\n",
        "        cropped_sequence = sequence[:, start_idx:end_idx]\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            cropped_mask = attention_mask[:, start_idx:end_idx]\n",
        "            return cropped_sequence, cropped_mask\n",
        "\n",
        "        return cropped_sequence, None\n",
        "\n",
        "    def random_temporal_mask(\n",
        "        self,\n",
        "        sequence: torch.Tensor,\n",
        "        attention_mask: torch.Tensor = None\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Apply random temporal masking to sequence\"\"\"\n",
        "        seq_length = sequence.size(1)\n",
        "\n",
        "        # Determine number of segments to mask\n",
        "        min_ratio, max_ratio = self.mask_ratio_range\n",
        "        mask_ratio = random.uniform(min_ratio, max_ratio)\n",
        "        num_masks = max(1, int(seq_length * mask_ratio))\n",
        "\n",
        "        # Create copy of sequence for masking\n",
        "        masked_sequence = sequence.clone()\n",
        "        if attention_mask is not None:\n",
        "            new_attention_mask = attention_mask.clone()\n",
        "\n",
        "        # Apply random masks\n",
        "        for _ in range(num_masks):\n",
        "            # Random mask length between 1 and 5 frames\n",
        "            mask_length = random.randint(1, min(5, seq_length // 10))\n",
        "            start_idx = random.randint(0, seq_length - mask_length)\n",
        "            end_idx = start_idx + mask_length\n",
        "\n",
        "            # Apply mask (set to zeros)\n",
        "            masked_sequence[:, start_idx:end_idx] = 0\n",
        "\n",
        "            if attention_mask is not None:\n",
        "                new_attention_mask[:, start_idx:end_idx] = 0\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            return masked_sequence, new_attention_mask\n",
        "\n",
        "        return masked_sequence, None\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from torch.optim import Adam\n",
        "# import numpy as np\n",
        "# from typing import Dict, Tuple\n",
        "# import wandb  # for logging\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(1, max_len, d_model)\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        print(f\"Input shape: {x.shape}\")\n",
        "        print(f\"Positional Encoding shape: {self.pe[:, :x.size(1)].shape}\")\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "# class SkeletonEmbedding(nn.Module):\n",
        "#     def __init__(self, d_model: int):\n",
        "#         super().__init__()\n",
        "#         # self.joint_embedding = nn.Linear(3, d_model)  # 3 for x,y,z coordinates\n",
        "#         self.joint_embedding = nn.Linear(60, d_model)  # 20*3 for x,y,z coordinates of 20 joints\n",
        "\n",
        "#         # Add a linear layer to reduce the concatenated joints to d_model\n",
        "#         # self.final_projection = nn.Linear(d_model * 20, d_model)\n",
        "\n",
        "#     # def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "#     #     # x shape: [batch_size, seq_len, num_joints, 3]\n",
        "#     #     print(f\"skeleton_emb shape before join: {x.shape}\")\n",
        "#     #     batch_size, seq_len, num_joints, _ = x.shape\n",
        "\n",
        "#     #     # Embed each joint\n",
        "#     #     x = x.view(batch_size * seq_len, num_joints, 3)\n",
        "#     #     print(f\"skeleton_emb shape after view: {x.shape}\")\n",
        "#     #     x = self.joint_embedding(x)  # [batch_size * seq_len, num_joints, d_model]\n",
        "#     #     print(f\"skeleton_emb shape after embedding: {x.shape}\")\n",
        "\n",
        "#     #     # Reshape back to original shape\n",
        "#     #     x = x.view(batch_size, seq_len, num_joints, -1)\n",
        "\n",
        "#     #     # Combine joint embeddings\n",
        "#     #     x = x.view(batch_size, seq_len, num_joints * x.size(-1))\n",
        "#     #     print(f\"skeleton_emb shape after join: {x.shape}\")\n",
        "\n",
        "#     #     # Project down to d_model dimension\n",
        "#     #     # x = self.final_projection(x)  # [batch_size, seq_len, d_model]\n",
        "#     #     return x\n",
        "\n",
        "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "#         # x shape: [batch_size, seq_len, num_joints, 3]\n",
        "#         print(f\"skeleton_emb shape before join: {x.shape}\")\n",
        "#         print(x[0][1])\n",
        "#         batch_size, seq_len, num_joints, coords = x.shape\n",
        "\n",
        "#         # Combine all joints first\n",
        "#         x = x.reshape(batch_size, seq_len, num_joints * coords)  # [batch_size, seq_len, 60]\n",
        "#         print(f\"skeleton_emb shape after combining joints: {x.shape}\")\n",
        "#         print(x[0][1])\n",
        "\n",
        "#         # Project to d_model\n",
        "#         x = self.joint_embedding(x)  # [batch_size, seq_len, d_model]\n",
        "#         print(f\"skeleton_emb shape after embedding: {x.shape}\")\n",
        "#         print(x[0][1])\n",
        "#         return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SkeletonEmbedding(nn.Module):\n",
        "    \"\"\"New approach: combine joints first, then embed\"\"\"\n",
        "    def __init__(self, d_model: int):\n",
        "        super().__init__()\n",
        "        print(\"d_model: \",d_model)\n",
        "        self.joint_embedding = nn.Linear(60, d_model)  # 20 joints * 3 coordinates = 60\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x shape: [batch_size, seq_len, num_joints, 3]\n",
        "        batch_size, seq_len, num_joints, coords = x.shape\n",
        "\n",
        "        # Combine all joints first\n",
        "        x = x.reshape(batch_size, seq_len, num_joints * coords)  # [batch_size, seq_len, 60]\n",
        "\n",
        "        # Project to d_model\n",
        "        x = self.joint_embedding(x)  # [batch_size, seq_len, d_model]\n",
        "        return x\n",
        "\n",
        "# class SkeletonTransformerTrainer:\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         model: SkeletonTransformer,\n",
        "#         train_loader: torch.utils.data.DataLoader,\n",
        "#         val_loader: torch.utils.data.DataLoader,\n",
        "#         learning_rate: float = 1e-4,\n",
        "#         weight_decay: float = 1e-4,\n",
        "#         use_wandb: bool = True\n",
        "#     ):\n",
        "#         self.model = model\n",
        "#         self.train_loader = train_loader\n",
        "#         self.val_loader = val_loader\n",
        "#         self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#         self.model = self.model.to(self.device)\n",
        "#         self.optimizer = Adam(\n",
        "#             model.parameters(),\n",
        "#             lr=learning_rate,\n",
        "#             weight_decay=weight_decay\n",
        "#         )\n",
        "\n",
        "#         self.contrastive_loss = NTXentLoss()\n",
        "#         self.classification_loss = nn.CrossEntropyLoss()\n",
        "#         self.use_wandb = use_wandb\n",
        "\n",
        "#     def train_epoch(self) -> Dict[str, float]:\n",
        "#         self.model.train()\n",
        "#         total_cont_loss = 0\n",
        "#         total_cls_loss = 0\n",
        "#         correct = 0\n",
        "#         total = 0\n",
        "\n",
        "#         for batch in self.train_loader:\n",
        "#             # Get batch data\n",
        "#             sequence = batch['sequence'].to(self.device)\n",
        "#             attention_mask = batch['attention_mask'].to(self.device)\n",
        "#             person_id = batch['person_id'].to(self.device)\n",
        "\n",
        "#             # Create augmented views\n",
        "#             aug1 = self.augment_sequence(sequence)\n",
        "#             aug2 = self.augment_sequence(sequence)\n",
        "\n",
        "#             # Forward pass\n",
        "#             proj1, logits1 = self.model(aug1, attention_mask)\n",
        "#             proj2, logits2 = self.model(aug2, attention_mask)\n",
        "\n",
        "#             # Calculate losses\n",
        "#             cont_loss = self.contrastive_loss(proj1, proj2)\n",
        "#             cls_loss = self.classification_loss(logits1, person_id)\n",
        "\n",
        "#             # Combined loss\n",
        "#             loss = cont_loss + cls_loss\n",
        "\n",
        "#             # Backward pass\n",
        "#             self.optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             self.optimizer.step()\n",
        "\n",
        "#             # Statistics\n",
        "#             total_cont_loss += cont_loss.item()\n",
        "#             total_cls_loss += cls_loss.item()\n",
        "\n",
        "#             pred = logits1.argmax(dim=1)\n",
        "#             correct += (pred == person_id).sum().item()\n",
        "#             total += person_id.size(0)\n",
        "\n",
        "#         return {\n",
        "#             'train_cont_loss': total_cont_loss / len(self.train_loader),\n",
        "#             'train_cls_loss': total_cls_loss / len(self.train_loader),\n",
        "#             'train_accuracy': correct / total\n",
        "#         }\n",
        "\n",
        "#     @torch.no_grad()\n",
        "#     def validate(self) -> Dict[str, float]:\n",
        "#         self.model.eval()\n",
        "#         total_loss = 0\n",
        "#         correct = 0\n",
        "#         total = 0\n",
        "\n",
        "#         for batch in self.val_loader:\n",
        "#             sequence = batch['sequence'].to(self.device)\n",
        "#             attention_mask = batch['attention_mask'].to(self.device)\n",
        "#             person_id = batch['person_id'].to(self.device)\n",
        "\n",
        "#             _, logits = self.model(sequence, attention_mask)\n",
        "#             loss = self.classification_loss(logits, person_id)\n",
        "\n",
        "#             total_loss += loss.item()\n",
        "#             pred = logits.argmax(dim=1)\n",
        "#             correct += (pred == person_id).sum().item()\n",
        "#             total += person_id.size(0)\n",
        "\n",
        "#         return {\n",
        "#             'val_loss': total_loss / len(self.val_loader),\n",
        "#             'val_accuracy': correct / total\n",
        "#         }\n",
        "\n",
        "#     def augment_sequence(self, sequence: torch.Tensor) -> torch.Tensor:\n",
        "#         \"\"\"Apply random augmentations to skeleton sequence\"\"\"\n",
        "#         # Example augmentations:\n",
        "#         # 1. Random rotation\n",
        "#         # 2. Random noise\n",
        "#         # 3. Random temporal crop/mask\n",
        "#         return sequence  # TODO: Implement actual augmentations\n",
        "\n",
        "#     def train(self, num_epochs: int):\n",
        "#         if self.use_wandb:\n",
        "#             wandb.init(project='skeleton-transformer')\n",
        "\n",
        "#         for epoch in range(num_epochs):\n",
        "#             train_metrics = self.train_epoch()\n",
        "#             val_metrics = self.validate()\n",
        "\n",
        "#             metrics = {**train_metrics, **val_metrics}\n",
        "\n",
        "#             if self.use_wandb:\n",
        "#                 wandb.log(metrics)\n",
        "\n",
        "#             print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "#             for k, v in metrics.items():\n",
        "#                 print(f\"{k}: {v:.4f}\")\n",
        "#             print()\n",
        "\n",
        "# Example usage\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Assuming you have your data loaders from the previous script\n",
        "#     data_dir = \"data\"\n",
        "#     train_loader, val_loader, test_loader = create_data_loaders(data_dir)\n",
        "\n",
        "#     # Get number of persons (classes) from the dataset\n",
        "#     num_classes = len(train_loader.dataset.person_ids)\n",
        "\n",
        "#     # Create model\n",
        "#     model = SkeletonTransformer(\n",
        "#         num_joints=20,\n",
        "#         d_model=256,\n",
        "#         nhead=8,\n",
        "#         num_encoder_layers=6,\n",
        "#         num_classes=num_classes\n",
        "#     )\n",
        "\n",
        "#     # Create trainer\n",
        "#     trainer = SkeletonTransformerTrainer(\n",
        "#         model=model,\n",
        "#         train_loader=train_loader,\n",
        "#         val_loader=val_loader\n",
        "#     )\n",
        "\n",
        "#     # Train model\n",
        "#     trainer.train(num_epochs=100)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VQnU_jWFoASY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkeletonTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_joints: int,\n",
        "        d_model: int = 256,\n",
        "        nhead: int = 8,\n",
        "        num_encoder_layers: int = 6,\n",
        "        dim_feedforward: int = 2048,\n",
        "        dropout: float = 0.1,\n",
        "        num_classes: int = None\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        # self.embedding = SkeletonEmbedding(d_model // num_joints)\n",
        "        self.embedding = SkeletonEmbedding(d_model)\n",
        "        # print(self.embedding.shape)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_encoder_layers\n",
        "        )\n",
        "\n",
        "        # Projection head for contrastive learning\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model, 128)  # 128-dimensional projection\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        if num_classes is not None:\n",
        "            self.classifier = nn.Linear(d_model, num_classes)\n",
        "        else:\n",
        "            self.classifier = None\n",
        "\n",
        "    def encode(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        attention_mask: torch.Tensor = None\n",
        "    ) -> torch.Tensor:\n",
        "        # x shape: [batch_size, seq_len, num_joints, 3]\n",
        "        x = self.embedding(x)  # [batch_size, seq_len, d_model]\n",
        "        # print(\"embeddings shape: \", x.shape)\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # Convert boolean mask to float attention mask\n",
        "            attention_mask = attention_mask.float()\n",
        "            attention_mask = attention_mask.masked_fill(\n",
        "                attention_mask == 0,\n",
        "                float('-inf')\n",
        "            )\n",
        "\n",
        "        encoded = self.transformer_encoder(x, src_key_padding_mask=attention_mask)\n",
        "        # Use [CLS] token (first token) as sequence representation\n",
        "        sequence_repr = encoded[:, 0]\n",
        "\n",
        "        return sequence_repr\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        attention_mask: torch.Tensor = None\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        sequence_repr = self.encode(x, attention_mask)\n",
        "        projection = self.projection(sequence_repr)\n",
        "\n",
        "        if self.classifier is not None:\n",
        "            logits = self.classifier(sequence_repr)\n",
        "        else:\n",
        "            logits = None\n",
        "\n",
        "        return projection, logits\n"
      ],
      "metadata": {
        "id": "FbLtd_PrqThw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NTXentLoss(nn.Module):\n",
        "    def __init__(self, temperature: float = 0.07):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        z_i: torch.Tensor,\n",
        "        z_j: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        batch_size = z_i.size(0)\n",
        "        z_i = F.normalize(z_i, dim=1)\n",
        "        z_j = F.normalize(z_j, dim=1)\n",
        "\n",
        "        representations = torch.cat([z_i, z_j], dim=0)\n",
        "        similarity_matrix = F.cosine_similarity(\n",
        "            representations.unsqueeze(1),\n",
        "            representations.unsqueeze(0),\n",
        "            dim=2\n",
        "        )\n",
        "\n",
        "        sim_ij = torch.diag(similarity_matrix, batch_size)\n",
        "        sim_ji = torch.diag(similarity_matrix, -batch_size)\n",
        "        positives = torch.cat([sim_ij, sim_ji], dim=0)\n",
        "\n",
        "        nominator = torch.exp(positives / self.temperature)\n",
        "        denominator = torch.sum(\n",
        "            torch.exp(similarity_matrix / self.temperature),\n",
        "            dim=1\n",
        "        )\n",
        "\n",
        "        all_losses = -torch.log(nominator / denominator)\n",
        "        loss = torch.sum(all_losses) / (2 * batch_size)\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "DCA0qVhSqbLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkeletonTransformerTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: SkeletonTransformer,\n",
        "        train_loader: torch.utils.data.DataLoader,\n",
        "        val_loader: torch.utils.data.DataLoader,\n",
        "        learning_rate: float = 1e-4,\n",
        "        weight_decay: float = 1e-4,\n",
        "        # use_wandb: bool = False,\n",
        "        save_dir: str = 'models'\n",
        "    ):\n",
        "\n",
        "        # ... (previous initialization code remains the same)\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.optimizer = Adam(\n",
        "            model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            weight_decay=weight_decay\n",
        "        )\n",
        "\n",
        "        self.contrastive_loss = NTXentLoss()\n",
        "        self.classification_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.save_dir = save_dir\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        self.augmenter = TemporalAugmenter()\n",
        "\n",
        "        # Initialize best metrics for model saving\n",
        "        self.best_val_accuracy = 0.0\n",
        "        self.best_epoch = 0\n",
        "\n",
        "    def augment_sequence(\n",
        "        self,\n",
        "        sequence: torch.Tensor,\n",
        "        attention_mask: torch.Tensor = None\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Apply temporal augmentations to sequence\"\"\"\n",
        "        # First apply random crop\n",
        "        sequence, attention_mask = self.augmenter.random_temporal_crop(\n",
        "            sequence, attention_mask\n",
        "        )\n",
        "\n",
        "        # Then apply random masking\n",
        "        sequence, attention_mask = self.augmenter.random_temporal_mask(\n",
        "            sequence, attention_mask\n",
        "        )\n",
        "\n",
        "        return sequence, attention_mask\n",
        "\n",
        "    def save_checkpoint(\n",
        "        self,\n",
        "        epoch: int,\n",
        "        metrics: Dict[str, float],\n",
        "        is_best: bool = False\n",
        "    ):\n",
        "        \"\"\"Save model checkpoint\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'metrics': metrics,\n",
        "            'best_val_accuracy': self.best_val_accuracy,\n",
        "            'best_epoch': self.best_epoch\n",
        "        }\n",
        "\n",
        "        # Save regular checkpoint\n",
        "        checkpoint_path = os.path.join(\n",
        "            self.save_dir,\n",
        "            f'checkpoint_epoch_{epoch}_{timestamp}.pt'\n",
        "        )\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "        # Save best model if applicable\n",
        "        if is_best:\n",
        "            best_path = os.path.join(self.save_dir, 'best_model.pt')\n",
        "            torch.save(checkpoint, best_path)\n",
        "\n",
        "        # Optionally save model architecture config\n",
        "        if epoch == 0:\n",
        "            config = {\n",
        "                'd_model': self.model.d_model,\n",
        "                'nhead': self.model.transformer_encoder.layers[0].nhead,\n",
        "                'num_encoder_layers': len(self.model.transformer_encoder.layers),\n",
        "                'dim_feedforward': self.model.transformer_encoder.layers[0].linear1.out_features,\n",
        "            }\n",
        "            config_path = os.path.join(self.save_dir, 'model_config.pt')\n",
        "            torch.save(config, config_path)\n",
        "\n",
        "    def load_checkpoint(self, checkpoint_path: str):\n",
        "        \"\"\"Load model checkpoint\"\"\"\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        self.best_val_accuracy = checkpoint['best_val_accuracy']\n",
        "        self.best_epoch = checkpoint['best_epoch']\n",
        "\n",
        "        return checkpoint['epoch']\n",
        "\n",
        "    def train_epoch(self) -> Dict[str, float]:\n",
        "        self.model.train()\n",
        "        total_cont_loss = 0\n",
        "        total_cls_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch in self.train_loader:\n",
        "            # Get batch data\n",
        "            sequence = batch['sequence'].to(self.device)\n",
        "            attention_mask = batch['attention_mask'].to(self.device)\n",
        "            person_id = batch['person_id'].to(self.device)\n",
        "\n",
        "            # Create augmented views\n",
        "            aug1, mask1 = self.augment_sequence(sequence, attention_mask)\n",
        "            aug2, mask2 = self.augment_sequence(sequence, attention_mask)\n",
        "\n",
        "            # Forward pass\n",
        "            proj1, logits1 = self.model(aug1, mask1)\n",
        "            proj2, logits2 = self.model(aug2, mask2)\n",
        "\n",
        "            # Calculate losses\n",
        "            cont_loss = self.contrastive_loss(proj1, proj2)\n",
        "            cls_loss = self.classification_loss(logits1, person_id)\n",
        "\n",
        "            # Combined loss\n",
        "            loss = cont_loss + cls_loss\n",
        "\n",
        "            # Backward pass\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            total_cont_loss += cont_loss.item()\n",
        "            total_cls_loss += cls_loss.item()\n",
        "\n",
        "            pred = logits1.argmax(dim=1)\n",
        "            correct += (pred == person_id).sum().item()\n",
        "            total += person_id.size(0)\n",
        "\n",
        "        return {\n",
        "            'train_cont_loss': total_cont_loss / len(self.train_loader),\n",
        "            'train_cls_loss': total_cls_loss / len(self.train_loader),\n",
        "            'train_accuracy': correct / total\n",
        "        }\n",
        "\n",
        "    def train(self, num_epochs: int, resume_path: str = None):\n",
        "        # if self.use_wandb:\n",
        "        #     wandb.init(project='skeleton-transformer')\n",
        "\n",
        "        start_epoch = 0\n",
        "        if resume_path is not None:\n",
        "            start_epoch = self.load_checkpoint(resume_path)\n",
        "            print(f\"Resumed training from epoch {start_epoch}\")\n",
        "\n",
        "        for epoch in range(start_epoch, num_epochs):\n",
        "            train_metrics = self.train_epoch()\n",
        "            val_metrics = self.validate()\n",
        "\n",
        "            metrics = {**train_metrics, **val_metrics}\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                # print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "                for k, v in metrics.items():\n",
        "                    print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "            # Check if this is the best model\n",
        "            is_best = False\n",
        "            if val_metrics['val_accuracy'] > self.best_val_accuracy:\n",
        "                self.best_val_accuracy = val_metrics['val_accuracy']\n",
        "                self.best_epoch = epoch\n",
        "                is_best = True\n",
        "\n",
        "            # Save checkpoint\n",
        "            self.save_checkpoint(epoch, metrics, is_best)\n",
        "\n",
        "            # if self.use_wandb:\n",
        "            #     wandb.log(metrics)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "            for k, v in metrics.items():\n",
        "                print(f\"{k}: {v:.4f}\")\n",
        "            if is_best:\n",
        "                print(\"New best model!\")\n",
        "            print()\n"
      ],
      "metadata": {
        "id": "zFPEVP17qmx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "# if __name__ == \"__main__\":\n",
        "# Create model and trainer\n",
        "model = SkeletonTransformer(\n",
        "    num_joints=20,\n",
        "    d_model=256,\n",
        "    nhead=8,\n",
        "    num_encoder_layers=6,\n",
        "    num_classes=164\n",
        ")\n",
        "\n",
        "trainer = SkeletonTransformerTrainer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    save_dir='skeleton_transformer_models'\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fy5vuIl4qq99",
        "outputId": "81a4be4c-68a7-4ffa-d220-9aea4d457d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d_model:  256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "trainer.train(\n",
        "    num_epochs=100,\n",
        "    resume_path=None  # Set to checkpoint path to resume training\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hmc6vF_eMKDn",
        "outputId": "c2a6b254-6891-4ea6-8244-2c4b7d57c7c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([32, 1482, 256])\n",
            "Positional Encoding shape: torch.Size([1, 1482, 256])\n"
          ]
        }
      ]
    }
  ]
}